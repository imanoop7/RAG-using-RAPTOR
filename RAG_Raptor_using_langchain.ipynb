{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SrEHJytn5ZV",
        "outputId": "185fd288-5b8b-4bfc-eaac-4a43d7f1a44c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.13)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.1.post1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.0.29)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: langchainhub in /usr/local/lib/python3.10/dist-packages (0.1.15)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.24)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.36)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.38)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.14.3)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0.20240311)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.110.0)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.29.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.10.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.17.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.24.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (29.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.36.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 825, in _parseNoCache\n",
            "    ret_tokens = ParseResults(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/results.py\", line 159, in __init__\n",
            "    def __init__(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1734, in isEnabledFor\n",
            "    _acquireLock()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 226, in _acquireLock\n",
            "    _lock.acquire()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "pip install -U langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmAWk0eNfkUh",
        "outputId": "34b14c50-f532-4af4-897a-695b36b6449a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1yv0WajTSPj"
      },
      "outputs": [],
      "source": [
        "pip install langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rXr-AorGoUmb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tiktoken\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "\n",
        "docs= PyPDFLoader(\"oops.pdf\")\n",
        "docs =docs.load_and_split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HNJYPSokr6Wo"
      },
      "outputs": [],
      "source": [
        "docs_texts = [d.page_content for d in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crQx2tq1o4-5",
        "outputId": "ebd458d8-e4e4-4666-9051-e7d0e45189f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num tokens in all context: 7135\n"
          ]
        }
      ],
      "source": [
        "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
        "d_reversed = list(reversed(d_sorted))\n",
        "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
        "    [doc.page_content for doc in d_reversed]\n",
        ")\n",
        "# print(\n",
        "#     \"Num tokens in all context: %s\"\n",
        "#     % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dENXws4wpDZi"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunk_size_tok = 2000\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
        ")\n",
        "texts_split = text_splitter.split_text(concatenated_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SXKPkDxvpnHA"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0HQTATdipLeg"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8mR_d27ppcBE"
      },
      "outputs": [],
      "source": [
        "model = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "E7--p5T1sHNd"
      },
      "outputs": [],
      "source": [
        "embd= GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GKZAchOopxJd"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
        "\n",
        "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
        "\n",
        "\n",
        "def global_cluster_embeddings(\n",
        "    embeddings: np.ndarray,\n",
        "    dim: int,\n",
        "    n_neighbors: Optional[int] = None,\n",
        "    metric: str = \"cosine\",\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - dim: The target dimensionality for the reduced space.\n",
        "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
        "                   If not provided, it defaults to the square root of the number of embeddings.\n",
        "    - metric: The distance metric to use for UMAP.\n",
        "\n",
        "    Returns:\n",
        "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
        "    \"\"\"\n",
        "    if n_neighbors is None:\n",
        "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
        "    return umap.UMAP(\n",
        "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
        "    ).fit_transform(embeddings)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "O2zzAEgdqd8b"
      },
      "outputs": [],
      "source": [
        "def local_cluster_embeddings(\n",
        "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - dim: The target dimensionality for the reduced space.\n",
        "    - num_neighbors: The number of neighbors to consider for each point.\n",
        "    - metric: The distance metric to use for UMAP.\n",
        "\n",
        "    Returns:\n",
        "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
        "    \"\"\"\n",
        "    return umap.UMAP(\n",
        "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
        "    ).fit_transform(embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "e_clRYoorLwC"
      },
      "outputs": [],
      "source": [
        "def get_optimal_clusters(\n",
        "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - max_clusters: The maximum number of clusters to consider.\n",
        "    - random_state: Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - An integer representing the optimal number of clusters found.\n",
        "    \"\"\"\n",
        "    max_clusters = min(max_clusters, len(embeddings))\n",
        "    n_clusters = np.arange(1, max_clusters)\n",
        "    bics = []\n",
        "    for n in n_clusters:\n",
        "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
        "        gm.fit(embeddings)\n",
        "        bics.append(gm.bic(embeddings))\n",
        "    return n_clusters[np.argmin(bics)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZGFLFKlQrPiE"
      },
      "outputs": [],
      "source": [
        "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
        "    \"\"\"\n",
        "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
        "    - random_state: Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing the cluster labels and the number of clusters determined.\n",
        "    \"\"\"\n",
        "    n_clusters = get_optimal_clusters(embeddings)\n",
        "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
        "    gm.fit(embeddings)\n",
        "    probs = gm.predict_proba(embeddings)\n",
        "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
        "    return labels, n_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Kxt3VJeXrSrU"
      },
      "outputs": [],
      "source": [
        "def perform_clustering(\n",
        "    embeddings: np.ndarray,\n",
        "    dim: int,\n",
        "    threshold: float,\n",
        ") -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
        "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - dim: The target dimensionality for UMAP reduction.\n",
        "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
        "\n",
        "    Returns:\n",
        "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
        "    \"\"\"\n",
        "    if len(embeddings) <= dim + 1:\n",
        "        # Avoid clustering when there's insufficient data\n",
        "        return [np.array([0]) for _ in range(len(embeddings))]\n",
        "\n",
        "    # Global dimensionality reduction\n",
        "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
        "    # Global clustering\n",
        "    global_clusters, n_global_clusters = GMM_cluster(\n",
        "        reduced_embeddings_global, threshold\n",
        "    )\n",
        "\n",
        "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
        "    total_clusters = 0\n",
        "\n",
        "    # Iterate through each global cluster to perform local clustering\n",
        "    for i in range(n_global_clusters):\n",
        "        # Extract embeddings belonging to the current global cluster\n",
        "        global_cluster_embeddings_ = embeddings[\n",
        "            np.array([i in gc for gc in global_clusters])\n",
        "        ]\n",
        "\n",
        "        if len(global_cluster_embeddings_) == 0:\n",
        "            continue\n",
        "        if len(global_cluster_embeddings_) <= dim + 1:\n",
        "            # Handle small clusters with direct assignment\n",
        "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
        "            n_local_clusters = 1\n",
        "        else:\n",
        "            # Local dimensionality reduction and clustering\n",
        "            reduced_embeddings_local = local_cluster_embeddings(\n",
        "                global_cluster_embeddings_, dim\n",
        "            )\n",
        "            local_clusters, n_local_clusters = GMM_cluster(\n",
        "                reduced_embeddings_local, threshold\n",
        "            )\n",
        "\n",
        "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
        "        for j in range(n_local_clusters):\n",
        "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
        "                np.array([j in lc for lc in local_clusters])\n",
        "            ]\n",
        "            indices = np.where(\n",
        "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
        "            )[1]\n",
        "            for idx in indices:\n",
        "                all_local_clusters[idx] = np.append(\n",
        "                    all_local_clusters[idx], j + total_clusters\n",
        "                )\n",
        "\n",
        "        total_clusters += n_local_clusters\n",
        "\n",
        "    return all_local_clusters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jnmgxIS9rcyi"
      },
      "outputs": [],
      "source": [
        "def embed(texts):\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of text documents.\n",
        "\n",
        "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
        "    that takes a list of texts and returns their embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: List[str], a list of text documents to be embedded.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
        "    \"\"\"\n",
        "    text_embeddings = embd.embed_documents(texts)\n",
        "    text_embeddings_np = np.array(text_embeddings)\n",
        "    return text_embeddings_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bLIlI2ZBrfY1"
      },
      "outputs": [],
      "source": [
        "def embed_cluster_texts(texts):\n",
        "    \"\"\"\n",
        "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
        "\n",
        "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
        "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: List[str], a list of text documents to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
        "    \"\"\"\n",
        "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
        "    cluster_labels = perform_clustering(\n",
        "        text_embeddings_np, 10, 0.1\n",
        "    )  # Perform clustering on the embeddings\n",
        "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
        "    df[\"text\"] = texts  # Store original texts\n",
        "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
        "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "5MPg6a2qrhvy"
      },
      "outputs": [],
      "source": [
        "def fmt_txt(df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Formats the text documents in a DataFrame into a single string.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the 'text' column with text documents to format.\n",
        "\n",
        "    Returns:\n",
        "    - A single string where all text documents are joined by a specific delimiter.\n",
        "    \"\"\"\n",
        "    unique_txt = df[\"text\"].tolist()\n",
        "    return \"--- --- \\n --- --- \".join(unique_txt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "9tdJwyfnrjuV"
      },
      "outputs": [],
      "source": [
        "def embed_cluster_summarize_texts(\n",
        "    texts: List[str], level: int\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
        "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
        "    the content within each cluster.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: A list of text documents to be processed.\n",
        "    - level: An integer parameter that could define the depth or detail of processing.\n",
        "\n",
        "    Returns:\n",
        "    - Tuple containing two DataFrames:\n",
        "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
        "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
        "         and the cluster identifiers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
        "    df_clusters = embed_cluster_texts(texts)\n",
        "\n",
        "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
        "    expanded_list = []\n",
        "\n",
        "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
        "    for index, row in df_clusters.iterrows():\n",
        "        for cluster in row[\"cluster\"]:\n",
        "            expanded_list.append(\n",
        "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
        "            )\n",
        "\n",
        "    # Create a new DataFrame from the expanded list\n",
        "    expanded_df = pd.DataFrame(expanded_list)\n",
        "\n",
        "    # Retrieve unique cluster identifiers for processing\n",
        "    all_clusters = expanded_df[\"cluster\"].unique()\n",
        "\n",
        "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
        "\n",
        "    # Summarization\n",
        "    template = \"\"\"Here is a sub-set of LangChain Expression Langauge doc.\n",
        "\n",
        "    LangChain Expression Langauge provides a way to compose chain in LangChain.\n",
        "\n",
        "    Give a detailed summary of the documentation provided.\n",
        "\n",
        "    Documentation:\n",
        "    {context}\n",
        "    \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    chain = prompt | model | StrOutputParser()\n",
        "\n",
        "    # Format text within each cluster for summarization\n",
        "    summaries = []\n",
        "    for i in all_clusters:\n",
        "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
        "        formatted_txt = fmt_txt(df_cluster)\n",
        "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
        "\n",
        "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
        "    df_summary = pd.DataFrame(\n",
        "        {\n",
        "            \"summaries\": summaries,\n",
        "            \"level\": [level] * len(summaries),\n",
        "            \"cluster\": list(all_clusters),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return df_clusters, df_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "6wBjA0gCroFj"
      },
      "outputs": [],
      "source": [
        "def recursive_embed_cluster_summarize(\n",
        "    texts: List[str], level: int = 1, n_levels: int = 3\n",
        ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
        "    the number of unique clusters becomes 1, storing the results at each level.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: List[str], texts to be processed.\n",
        "    - level: int, current recursion level (starts at 1).\n",
        "    - n_levels: int, maximum depth of recursion.\n",
        "\n",
        "    Returns:\n",
        "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
        "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
        "    \"\"\"\n",
        "    results = {}  # Dictionary to store results at each level\n",
        "\n",
        "    # Perform embedding, clustering, and summarization for the current level\n",
        "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
        "\n",
        "    # Store the results of the current level\n",
        "    results[level] = (df_clusters, df_summary)\n",
        "\n",
        "    # Determine if further recursion is possible and meaningful\n",
        "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
        "    if level < n_levels and unique_clusters > 1:\n",
        "        # Use summaries as the input texts for the next level of recursion\n",
        "        new_texts = df_summary[\"summaries\"].tolist()\n",
        "        next_level_results = recursive_embed_cluster_summarize(\n",
        "            new_texts, level + 1, n_levels\n",
        "        )\n",
        "\n",
        "        # Merge the results from the next level into the current results dictionary\n",
        "        results.update(next_level_results)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgxjwVrdrtFB",
        "outputId": "80166306-bd72-4a34-eb35-b3b13a1f1260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Generated 4 clusters--\n",
            "--Generated 1 clusters--\n"
          ]
        }
      ],
      "source": [
        "leaf_texts = docs_texts\n",
        "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "LdCwHN5wrwp2"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Initialize all_texts with leaf_texts\n",
        "all_texts = leaf_texts.copy()\n",
        "\n",
        "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
        "for level in sorted(results.keys()):\n",
        "    # Extract summaries from the current level's DataFrame\n",
        "    summaries = results[level][1][\"summaries\"].tolist()\n",
        "    # Extend all_texts with the summaries from the current level\n",
        "    all_texts.extend(summaries)\n",
        "\n",
        "# Now, use all_texts to build the vectorstore with Chroma\n",
        "vectorstore = Chroma.from_texts(texts=all_texts, embedding=embd)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Ow97j5RBtAD0",
        "outputId": "c67dd8b4-cf43-4be0-db90-d05300ae54e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'OOPS stands for Object-Oriented Programming System. In OOPS, programs are organized as a collection of objects, where each object is an instance of a class. OOPS is based on the concepts of abstraction, encapsulation, inheritance, and polymorphism.'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "rag_chain.invoke(\"what is OOPS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRG9JjA2tEoK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
